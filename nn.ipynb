{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from timer import Timer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuronMax = 1\n",
    "neuronMin = -1\n",
    "neuronDtype = np.float32\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    @staticmethod\n",
    "    def sigmoid(x, derivative=False):\n",
    "        if derivative:\n",
    "            return x * (1 - x)\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    @staticmethod\n",
    "    def relu(x, derivative=False):\n",
    "        if derivative:\n",
    "            return 1.0 * (x > 0)\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    @staticmethod\n",
    "    def leakyRelu(x, derivative=False):\n",
    "        if derivative:\n",
    "            return 1.0 * (x > 0) + 0.01 * (x <= 0)\n",
    "        return np.maximum(0.01 * x, x)\n",
    "\n",
    "    @staticmethod\n",
    "    def absolute(x, derivative=False):\n",
    "        if derivative:\n",
    "            return 1 if x > 0 else -1\n",
    "        return np.abs(x)\n",
    "\n",
    "\n",
    "    # @staticmethod\n",
    "    # def softmax(x):\n",
    "    #     exps = np.exp(x - np.max(x))\n",
    "    #     return exps / np.sum(exps, axis=1, keepdims=True)\n",
    "\n",
    "    # @staticmethod\n",
    "    # def softmaxDerivative(x):\n",
    "    #     return x * (1 - x)\n",
    "    @staticmethod\n",
    "    def mseLoss(errorVector, derivative=False):\n",
    "        if derivative:\n",
    "            return errorVector\n",
    "        return np.mean(np.square(errorVector))*0.5\n",
    "\n",
    "    def __init__(self, layerSizes, activationFunctions):\n",
    "        self.layerSizes = layerSizes\n",
    "        self.activationFunctions = activationFunctions\n",
    "        self.layers = [\n",
    "            np.random.uniform(low=neuronMin, high=neuronMax, size=layerSizes[i]).astype(neuronDtype)\n",
    "            for i in range(len(layerSizes))\n",
    "        ]\n",
    "        self.weights = [\n",
    "            np.random.uniform(\n",
    "                low=neuronMin, high=neuronMax, size=layerSizes[i] * layerSizes[i + 1]\n",
    "            ).astype(neuronDtype).reshape(layerSizes[i], layerSizes[i + 1])\n",
    "            for i in range(len(layerSizes) - 1)\n",
    "        ]\n",
    "        self.biases = [\n",
    "            np.random.uniform(low=neuronMin, high=neuronMax, size=layerSizes[i + 1]).astype(\n",
    "                neuronDtype\n",
    "            )\n",
    "            for i in range(len(layerSizes) - 1)\n",
    "        ]\n",
    "\n",
    "    def loadInput(self, input):\n",
    "        self.layers[0] = input.astype(neuronDtype)\n",
    "\n",
    "    def readOutput(self):\n",
    "        return self.layers[-1]\n",
    "\n",
    "    def forward(self):\n",
    "        for i in range(len(self.weights)):\n",
    "            self.layers[i + 1] = self.activationFunctions[i](\n",
    "                np.dot(self.layers[i], self.weights[i]) + self.biases[i]\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "    def backpropagation(self, target, learning_rate, lossFunction):\n",
    "        output_error = lossFunction(target - self.layers[-1], derivative=True)\n",
    "\n",
    "        for i in range(len(self.layers) - 2, -1, -1):\n",
    "            # Calculate the derivative of the activation function\n",
    "            derivative = self.activationFunctions[i](self.layers[i + 1], derivative=True)\n",
    "\n",
    "            # Calculate the error of the current layer\n",
    "            layer_error = np.dot(output_error * derivative, self.weights[i].T) * learning_rate\n",
    "\n",
    "            # Calculate the adjustments for the weights and biases\n",
    "            weight_adjustments = np.outer(self.layers[i], output_error)\n",
    "            bias_adjustments = output_error\n",
    "\n",
    "            # Update the weights and biases\n",
    "            self.weights[i] += weight_adjustments * learning_rate\n",
    "            self.biases[i] += bias_adjustments * learning_rate\n",
    "\n",
    "            # Set the output error for the next iteration\n",
    "            # np.normalize(layer_error)\n",
    "            output_error = layer_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = NeuralNetwork([5_000, 40, 40, 3], [NeuralNetwork.leakyRelu, NeuralNetwork.leakyRelu, NeuralNetwork.leakyRelu])\n",
    "nnTimer = Timer(precision=3)\n",
    "target = np.array([2, 3, 4])\n",
    "input = np.random.uniform(low = 0, high = 1, size = 5_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[89.33454    55.644417   -0.56158143]\n"
     ]
    }
   ],
   "source": [
    "x.loadInput(input)\n",
    "x.forward()\n",
    "print(x.readOutput())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[89.33454    55.644417   -0.56158143] 1736.59412567521\n",
      "[40.728992   26.345533   -0.53619474] 344.25430804065354\n",
      "[19.174616   13.352728   -0.51094913] 70.41584446662348\n",
      "[ 9.616208   7.590993  -0.4858441] 16.534439810618093\n",
      "[ 5.3774467  5.035905  -0.4608788] 5.908582359243941\n",
      "[ 3.4977589   3.9028394  -0.43605223] 3.7894933424687367\n",
      "[ 2.6642175  3.4003737 -0.4113639] 3.343602567221851\n",
      "[ 2.294536    3.1775608  -0.38681293] 3.2270678340233965\n",
      "[ 2.1306233   3.078728   -0.36239862] 3.175630384538721\n",
      "[ 2.057948   3.0349114 -0.3381201] 3.1373104693687544\n",
      "[ 2.025679    3.0154867  -0.31397703] 3.1018828492607113\n",
      "[ 2.0113888   3.0068653  -0.28996813] 3.067333903006208\n",
      "[ 2.0050526   3.003043   -0.26609278] 3.0332637286688944\n",
      "[ 2.002245    3.0013568  -0.24235028] 2.9995904635743176\n",
      "[ 2.0009823   3.0006053  -0.21874008] 2.966294859785171\n",
      "[ 2.000452   3.0002582 -0.1952611] 2.9333693362747244\n",
      "[ 2.0002003   3.000117   -0.17191283] 2.9008094581182107\n",
      "[ 2.000082    3.0000675  -0.14869456] 2.868611093814416\n",
      "[ 2.0000324  3.000018  -0.1256054] 2.836769992332616\n",
      "[ 2.0000248   3.0000026  -0.10264482] 2.8052824247239836\n",
      "[ 2.0000172   3.0000026  -0.07981215] 2.7741445356077055\n",
      "[ 2.0000095   3.0000026  -0.05710655] 2.7433522607505836\n",
      "[ 2.0000095   3.0000026  -0.03452707] 2.712901447546652\n",
      "[ 2.0000095   3.0000026  -0.01207343] 2.682788873261553\n",
      "[2.0000095 3.0000026 1.0255382] 1.474570493910979\n",
      "[2.0000095 3.0000026 2.6809177] 0.28999633484872334\n",
      "[2.0000095 3.0000026 3.415054 ] 0.05702695434405314\n",
      "[2.0000095 3.0000026 3.7406178] 0.011213191772725395\n",
      "[2.0000095 3.0000026 3.8849454] 0.0022062604633011538\n",
      "[2.0000095 3.0000026 3.9489992] 0.00043351418611337067\n",
      "[2.0000095 3.0000026 3.9773715] 8.534186351501678e-05\n",
      "[2.0000095 3.0000026 3.9899678] 1.6774112102287592e-05\n",
      "[2.0000095 2.999995  3.9955509] 3.2991443011572605e-06\n",
      "[2.0000095 3.0000026 3.9980285] 6.478073260041128e-07\n",
      "[2.0000095 3.0000026 3.999129 ] 1.264399429601326e-07\n",
      "[2.0000095 2.999995  3.9995983] 2.6917878888828756e-08\n",
      "[2.0000095 3.0000026 3.9998271] 4.996024927095277e-09\n",
      "[2.0000095 3.0000026 3.999934 ] 7.432277016050648e-10\n",
      "[2.0000095 2.999995  3.999976 ] 1.1597952228233528e-10\n",
      "[2.0000095 3.0000026 3.999976 ] 1.1294787327642553e-10\n",
      "[2.0000095 3.0000026 3.9999912] 2.927436071331613e-11\n",
      "[2.0000095 2.999995  3.9999912] 3.230600971922589e-11\n",
      "[2.0000095 3.0000026 3.9999912] 2.927436071331613e-11\n",
      "[2.0000095 3.0000026 3.9999912] 2.927436071331613e-11\n",
      "[2.0000095 2.999995  3.9999988] 1.9573083894404892e-11\n",
      "[2.0000095 3.0000026 3.9999988] 1.6541434888495132e-11\n",
      "[2.0000095 3.0000026 3.9999988] 1.6541434888495132e-11\n",
      "[2.000002  2.999995  3.9999988] 5.021168666038041e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  2.999995  3.9999988] 5.021168666038041e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  2.999995  3.9999988] 5.021168666038041e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  2.999995  3.9999988] 5.021168666038041e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  2.999995  3.9999988] 5.021168666038041e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  2.999995  3.9999988] 5.021168666038041e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  2.999995  3.9999988] 5.021168666038041e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  2.999995  3.9999988] 5.021168666038041e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  2.999995  3.9999988] 5.021168666038041e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  2.999995  3.9999988] 5.021168666038041e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  2.999995  3.9999988] 5.021168666038041e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  2.999995  3.9999988] 5.021168666038041e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  2.999995  3.9999988] 5.021168666038041e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  2.999995  3.9999988] 5.021168666038041e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  2.999995  3.9999988] 5.021168666038041e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  2.999995  3.9999988] 5.021168666038041e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  2.999995  3.9999988] 5.021168666038041e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  2.999995  3.9999988] 5.021168666038041e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  2.999995  3.9999988] 5.021168666038041e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  2.999995  3.9999988] 5.021168666038041e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  2.999995  3.9999988] 5.021168666038041e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  2.999995  3.9999988] 5.021168666038041e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  2.999995  3.9999988] 5.021168666038041e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  2.999995  3.9999988] 5.021168666038041e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  2.999995  3.9999988] 5.021168666038041e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  2.999995  3.9999988] 5.021168666038041e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  2.999995  3.9999988] 5.021168666038041e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  2.999995  3.9999988] 5.021168666038041e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  2.999995  3.9999988] 5.021168666038041e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  2.999995  3.9999988] 5.021168666038041e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  2.999995  3.9999988] 5.021168666038041e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  2.999995  3.9999988] 5.021168666038041e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  2.999995  3.9999988] 5.021168666038041e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  3.0000026 4.0000067] 9.180212146020494e-12\n",
      "[2.000002  2.999995  3.9999988] 5.021168666038041e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  2.999995  3.9999988] 5.021168666038041e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  2.999995  4.0000067] 1.2211861151930256e-11\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  2.999995  3.9999988] 5.021168666038041e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  2.999995  3.9999988] 5.021168666038041e-12\n",
      "[2.000002  3.0000026 4.0000067] 9.180212146020494e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  2.999995  3.9999988] 5.021168666038041e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  2.999995  3.9999988] 5.021168666038041e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  3.0000026 4.0000067] 9.180212146020494e-12\n",
      "[2.000002  2.999995  3.9999988] 5.021168666038041e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  2.999995  3.9999988] 5.021168666038041e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  2.999995  4.0000067] 1.2211861151930256e-11\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  2.999995  3.9999988] 5.021168666038041e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  2.999995  3.9999988] 5.021168666038041e-12\n",
      "[2.000002  3.0000026 4.0000067] 9.180212146020494e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  2.999995  3.9999988] 5.021168666038041e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  2.999995  3.9999988] 5.021168666038041e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  3.0000026 4.0000067] 9.180212146020494e-12\n",
      "[2.000002  2.999995  3.9999988] 5.021168666038041e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  2.999995  3.9999988] 5.021168666038041e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  2.999995  4.0000067] 1.2211861151930256e-11\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  2.999995  3.9999988] 5.021168666038041e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "[2.000002  3.0000026 3.9999988] 1.9895196601282805e-12\n",
      "Total time: 0.547 s\n",
      "Total calls: 200\n",
      "Average time: 0.002735 s\n"
     ]
    }
   ],
   "source": [
    "# learning_rate = 0.00001\n",
    "# for i in range(200):\n",
    "#     nnTimer.tic()\n",
    "#     x.loadInput(input)\n",
    "#     x.forward()\n",
    "#     x.backpropagation(target, learning_rate)\n",
    "#     print(x.readOutput())\n",
    "#     nnTimer.toc()\n",
    "# nnTimer.stats()\n",
    "learning_rate = 0.00001\n",
    "for i in range(200):\n",
    "    nnTimer.tic()\n",
    "    x.loadInput(input)\n",
    "    x.forward()\n",
    "    x.backpropagation(target, learning_rate, NeuralNetwork.mseLoss)\n",
    "    print(x.readOutput(), NeuralNetwork.mseLoss(target - x.readOutput()))\n",
    "    nnTimer.toc()\n",
    "nnTimer.stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "size = 5\n",
    "padding = 3\n",
    "\n",
    "a = np.zeros((size + padding * 2, size + padding * 2))\n",
    "\n",
    "for i in range(padding, size + padding):\n",
    "    for j in range(padding, size + padding):\n",
    "        a[i, j] = i - padding + (j - padding) * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0., 10., 20., 30., 40.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1., 11., 21., 31., 41.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  2., 12., 22., 32., 42.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  3., 13., 23., 33., 43.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  4., 14., 24., 34., 44.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
