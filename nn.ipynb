{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from timer import Timer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxFloat16 = np.finfo(np.float16).max\n",
    "neuronMax = 10\n",
    "neuronMin = -10\n",
    "neuronDtype = np.float32\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    @staticmethod\n",
    "    def sigmoid(x, derivative=False):\n",
    "        if derivative:\n",
    "            return x * (1 - x)\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    @staticmethod\n",
    "    def relu(x, derivative=False):\n",
    "        if derivative:\n",
    "            return 1.0 #* (x > 0)\n",
    "        return x #np.maximum(0, x)\n",
    "\n",
    "\n",
    "    # @staticmethod\n",
    "    # def softmax(x):\n",
    "    #     exps = np.exp(x - np.max(x))\n",
    "    #     return exps / np.sum(exps, axis=1, keepdims=True)\n",
    "\n",
    "    # @staticmethod\n",
    "    # def softmaxDerivative(x):\n",
    "    #     return x * (1 - x)\n",
    "\n",
    "    def __init__(self, layerSizes, activationFunctions):\n",
    "        self.layerSizes = layerSizes\n",
    "        self.activationFunctions = activationFunctions\n",
    "        self.layers = [\n",
    "            np.random.uniform(low=neuronMin, high=neuronMax, size=layerSizes[i]).astype(neuronDtype)\n",
    "            for i in range(len(layerSizes))\n",
    "        ]\n",
    "        self.weights = [\n",
    "            np.random.uniform(\n",
    "                low=neuronMin, high=neuronMax, size=layerSizes[i] * layerSizes[i + 1]\n",
    "            ).astype(neuronDtype).reshape(layerSizes[i], layerSizes[i + 1])\n",
    "            for i in range(len(layerSizes) - 1)\n",
    "        ]\n",
    "        self.biases = [\n",
    "            np.random.uniform(low=neuronMin, high=neuronMax, size=layerSizes[i + 1]).astype(\n",
    "                neuronDtype\n",
    "            )\n",
    "            for i in range(len(layerSizes) - 1)\n",
    "        ]\n",
    "\n",
    "    def loadInput(self, input):\n",
    "        self.layers[0] = input.astype(neuronDtype)\n",
    "\n",
    "    def readOutput(self):\n",
    "        return self.layers[-1]\n",
    "\n",
    "    def forward(self):\n",
    "        for i in range(len(self.weights)):\n",
    "            self.layers[i + 1] = self.activationFunctions[i](\n",
    "                np.dot(self.layers[i], self.weights[i]) + self.biases[i]\n",
    "            )\n",
    "\n",
    "    def mseLoss(self, target, layer_no):\n",
    "        return np.sum(np.square(target - self.layers[layer_no]))/2\n",
    "\n",
    "    def backpropagation(self, target, learning_rate):\n",
    "        output_error = target - self.layers[-1]\n",
    "\n",
    "        for i in range(len(self.layers) - 2, -1, -1):\n",
    "            # Calculate the derivative of the activation function\n",
    "            derivative = self.activationFunctions[i](self.layers[i + 1], derivative=True)\n",
    "\n",
    "            # Calculate the error of the current layer\n",
    "            layer_error = np.dot(output_error * derivative, self.weights[i].T)\n",
    "\n",
    "            # Calculate the adjustments for the weights and biases\n",
    "            weight_adjustments = np.outer(self.layers[i], output_error)\n",
    "            bias_adjustments = output_error\n",
    "\n",
    "            # Update the weights and biases\n",
    "            self.weights[i] += weight_adjustments * learning_rate\n",
    "            self.biases[i] += bias_adjustments * learning_rate\n",
    "\n",
    "            # Set the output error for the next iteration\n",
    "            output_error = layer_error * learning_rate#derivative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = NeuralNetwork([6, 5, 3], [NeuralNetwork.relu, NeuralNetwork.relu])\n",
    "# nnTimer = Timer(precision=3)\n",
    "# target = np.array([2, 3, 4])\n",
    "# input = np.random.rand(6)\n",
    "\n",
    "\n",
    "# x = NeuralNetwork([4_107, 40, 40, 3], [NeuralNetwork.relu, NeuralNetwork.relu, NeuralNetwork.relu])\n",
    "# nnTimer = Timer(precision=3)\n",
    "# target = np.array([2, 3, 4])\n",
    "# input = np.random.rand(4_107)\n",
    "\n",
    "x = NeuralNetwork([300_000, 40, 40, 3], [NeuralNetwork.relu, NeuralNetwork.relu, NeuralNetwork.relu])\n",
    "nnTimer = Timer(precision=3)\n",
    "target = np.array([2, 3, 4])\n",
    "input = np.random.rand(300_000)\n",
    "\n",
    "\n",
    "# x = NeuralNetwork([12, 10, 10, 25], [NeuralNetwork.relu, NeuralNetwork.relu, NeuralNetwork.relu])\n",
    "# nnTimer = Timer(precision=3)\n",
    "# target = np.arange(25)\n",
    "# input = np.random.rand(12)\n",
    "\n",
    "\n",
    "\n",
    "# for i in range(1):\n",
    "#     nnTimer.tic()\n",
    "#     x.loadInput(np.random.rand(30_000))\n",
    "#     x.forward()\n",
    "#     nnTimer.toc()\n",
    "#     print(x.readOutput())\n",
    "# nnTimer.stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3979031.8   -903105.25  2305689.2 ]\n"
     ]
    }
   ],
   "source": [
    "x.loadInput(input)\n",
    "x.forward()\n",
    "print(x.readOutput())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\milos\\AppData\\Local\\Temp\\ipykernel_20112\\3886743066.py:80: RuntimeWarning: overflow encountered in add\n",
      "  self.weights[i] += weight_adjustments * learning_rate\n",
      "C:\\Users\\milos\\AppData\\Local\\Temp\\ipykernel_20112\\3886743066.py:81: RuntimeWarning: overflow encountered in add\n",
      "  self.biases[i] += bias_adjustments * learning_rate\n",
      "C:\\Users\\milos\\AppData\\Local\\Temp\\ipykernel_20112\\3886743066.py:80: RuntimeWarning: overflow encountered in add\n",
      "  self.weights[i] += weight_adjustments * learning_rate\n",
      "C:\\Users\\milos\\AppData\\Local\\Temp\\ipykernel_20112\\3886743066.py:81: RuntimeWarning: overflow encountered in add\n",
      "  self.biases[i] += bias_adjustments * learning_rate\n",
      "C:\\Users\\milos\\AppData\\Local\\Temp\\ipykernel_20112\\3886743066.py:80: RuntimeWarning: overflow encountered in add\n",
      "  self.weights[i] += weight_adjustments * learning_rate\n",
      "C:\\Users\\milos\\AppData\\Local\\Temp\\ipykernel_20112\\3886743066.py:81: RuntimeWarning: overflow encountered in add\n",
      "  self.biases[i] += bias_adjustments * learning_rate\n",
      "C:\\Users\\milos\\AppData\\Local\\Temp\\ipykernel_20112\\3886743066.py:80: RuntimeWarning: overflow encountered in add\n",
      "  self.weights[i] += weight_adjustments * learning_rate\n",
      "C:\\Users\\milos\\AppData\\Local\\Temp\\ipykernel_20112\\3886743066.py:81: RuntimeWarning: overflow encountered in add\n",
      "  self.biases[i] += bias_adjustments * learning_rate\n",
      "C:\\Users\\milos\\AppData\\Local\\Temp\\ipykernel_20112\\3886743066.py:80: RuntimeWarning: overflow encountered in add\n",
      "  self.weights[i] += weight_adjustments * learning_rate\n",
      "C:\\Users\\milos\\AppData\\Local\\Temp\\ipykernel_20112\\3886743066.py:80: RuntimeWarning: overflow encountered in add\n",
      "  self.weights[i] += weight_adjustments * learning_rate\n",
      "C:\\Users\\milos\\AppData\\Local\\Temp\\ipykernel_20112\\3886743066.py:81: RuntimeWarning: overflow encountered in add\n",
      "  self.biases[i] += bias_adjustments * learning_rate\n",
      "C:\\Users\\milos\\AppData\\Local\\Temp\\ipykernel_20112\\3886743066.py:80: RuntimeWarning: overflow encountered in add\n",
      "  self.weights[i] += weight_adjustments * learning_rate\n",
      "C:\\Users\\milos\\AppData\\Local\\Temp\\ipykernel_20112\\3886743066.py:81: RuntimeWarning: overflow encountered in add\n",
      "  self.biases[i] += bias_adjustments * learning_rate\n",
      "C:\\Users\\milos\\AppData\\Local\\Temp\\ipykernel_20112\\3886743066.py:80: RuntimeWarning: invalid value encountered in add\n",
      "  self.weights[i] += weight_adjustments * learning_rate\n",
      "C:\\Users\\milos\\AppData\\Local\\Temp\\ipykernel_20112\\3886743066.py:81: RuntimeWarning: invalid value encountered in add\n",
      "  self.biases[i] += bias_adjustments * learning_rate\n",
      "C:\\Users\\milos\\AppData\\Roaming\\Python\\Python312\\site-packages\\numpy\\core\\_methods.py:118: RuntimeWarning: invalid value encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "C:\\Users\\milos\\AppData\\Local\\Temp\\ipykernel_20112\\3886743066.py:80: RuntimeWarning: overflow encountered in add\n",
      "  self.weights[i] += weight_adjustments * learning_rate\n",
      "C:\\Users\\milos\\AppData\\Local\\Temp\\ipykernel_20112\\3886743066.py:81: RuntimeWarning: overflow encountered in add\n",
      "  self.biases[i] += bias_adjustments * learning_rate\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = pd.DataFrame(\n",
    "    columns=[\n",
    "        \"trialID\",\n",
    "        \"ticTime\",\n",
    "        \"totalTime\",\n",
    "        \"learningRates\",\n",
    "        \"inputSize\",\n",
    "        \"target\",\n",
    "        \"outputs\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "learning_rates = [0.1, 0.01, 0.001, 0.0001, 0.0000000000001, 0.0000000000001]\n",
    "inputSizes = [12, 500, 5_000, 50_000, 500_000]\n",
    "\n",
    "\n",
    "testCase = 0\n",
    "for learning_rate in learning_rates:\n",
    "    for inputSize in inputSizes:\n",
    "        x = NeuralNetwork(\n",
    "            [inputSize, 40, 40, 3],\n",
    "            [NeuralNetwork.relu, NeuralNetwork.relu, NeuralNetwork.relu],\n",
    "        )\n",
    "        nnTimer = Timer(precision=3)\n",
    "        target = np.arange(3)\n",
    "        input = np.random.rand(inputSize)\n",
    "        outputs = []\n",
    "\n",
    "        i = 0\n",
    "        while (\n",
    "            i < 10_000\n",
    "            and nnTimer.getElapsedTime() < 100_000\n",
    "            and (x.readOutput().mean() not in (np.inf, -np.inf, np.nan))\n",
    "            # and sum([ (res < targ * 1.1 and res > targ * 1.1 ) for res, targ in zip(x.readOutput(), target)])\n",
    "        ):\n",
    "            nnTimer.tic()\n",
    "            x.loadInput(input)\n",
    "            x.forward()\n",
    "            x.backpropagation(target, learning_rate)\n",
    "            nnTimer.toc()\n",
    "            outputs.append(x.readOutput())\n",
    "            i += 1\n",
    "\n",
    "        results.loc[testCase] = [\n",
    "            testCase,\n",
    "            nnTimer.tic_time,\n",
    "            nnTimer.total_time,\n",
    "            learning_rate,\n",
    "            inputSize,\n",
    "            target,\n",
    "            outputs,\n",
    "        ]\n",
    "\n",
    "        # (\n",
    "        #     {\n",
    "        #         \"trialID\": testCase,\n",
    "        #         \"ticTime\": nnTimer.tic_time,\n",
    "        #         \"totalTime\": nnTimer.total_time,\n",
    "        #         \"learningRates\": learning_rate,\n",
    "        #         \"inputSize\": inputSize,\n",
    "        #         \"target\": target,\n",
    "        #         \"outputs\": outputs,\n",
    "        #     },\n",
    "        # )\n",
    "\n",
    "        if testCase % 10 == 0:\n",
    "            with open(f\"profiling/results{testCase/10}.pkl\", \"wb\") as f:\n",
    "                pickle.dump(results, f)\n",
    "\n",
    "        testCase += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 0.705 s\n",
      "Total calls: 20\n",
      "Average time: 0.03525 s\n"
     ]
    }
   ],
   "source": [
    "nnTimer.stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0000000000001\n",
    "for i in range(20):\n",
    "    x.backpropagation(target, learning_rate)\n",
    "    nnTimer.tic()\n",
    "    x.loadInput(input)\n",
    "    x.forward()\n",
    "    print(x.readOutput())\n",
    "    nnTimer.toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "215.703125"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.process_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "216.671875"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.process_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19703"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timer = Timer(precision=3)\n",
    "timer.tic()\n",
    "x = 0\n",
    "for i in range(1000000):\n",
    "    x+=x+1\n",
    "timer.toc()\n",
    "timer.getElapsedTime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(\n",
    "    columns=[\n",
    "        \"trialID\",\n",
    "        \"ticTime\",\n",
    "        \"totalTime\",\n",
    "        \"learningRates\",\n",
    "        \"inputSize\",\n",
    "        \"target\",\n",
    "        \"outputs\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "iloc cannot enlarge its target object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[363], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mresults\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m      2\u001b[0m     {\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrialID\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mticTime\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.1\u001b[39m,\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotalTime\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.2\u001b[39m,\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearningRates\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.1\u001b[39m,\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputSize\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m12\u001b[39m,\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m3\u001b[39m),\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m3\u001b[39m),\n\u001b[0;32m     10\u001b[0m     },\n\u001b[0;32m     11\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\indexing.py:908\u001b[0m, in \u001b[0;36m_LocationIndexer.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    906\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001b[0;32m    907\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_setitem_indexer(key)\n\u001b[1;32m--> 908\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_has_valid_setitem_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    910\u001b[0m iloc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miloc\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39miloc\n\u001b[0;32m    911\u001b[0m iloc\u001b[38;5;241m.\u001b[39m_setitem_with_indexer(indexer, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\indexing.py:1646\u001b[0m, in \u001b[0;36m_iLocIndexer._has_valid_setitem_indexer\u001b[1;34m(self, indexer)\u001b[0m\n\u001b[0;32m   1644\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_integer(i):\n\u001b[0;32m   1645\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(ax):\n\u001b[1;32m-> 1646\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miloc cannot enlarge its target object\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1647\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(i, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m   1648\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miloc cannot enlarge its target object\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: iloc cannot enlarge its target object"
     ]
    }
   ],
   "source": [
    "results.loc[0] = (\n",
    "    {\n",
    "        \"trialID\": 0,\n",
    "        \"ticTime\": 0.1,\n",
    "        \"totalTime\": 0.2,\n",
    "        \"learningRates\": 0.1,\n",
    "        \"inputSize\": 12,\n",
    "        \"target\": np.arange(3),\n",
    "        \"outputs\": np.random.rand(3),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
