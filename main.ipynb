{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timer import *\n",
    "from utils import  mkdir_if_not_exist\n",
    "import globals\n",
    "\n",
    "from sheep import Sheep\n",
    "from wolf import Wolf\n",
    "from grass import Grass\n",
    "#grass has no brain, so it is not imported\n",
    "\n",
    "from game import Game\n",
    "from display import Display\n",
    "import neuralNetwork as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Sheep.add_brain()\n",
    "Wolf.add_brain()\n",
    "\n",
    "# mkdir_if_not_exist(f\"learned\")\n",
    "# Sheep.brain.save(f\"learned/starvingSheep.pkl\")\n",
    "# Wolf.brain.save(f\"learned/starvingWolf.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 0\n",
      "Game 1\n",
      "Game 2\n",
      "Game 3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGame \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m game\u001b[38;5;241m.\u001b[39msetup()\n\u001b[1;32m----> 9\u001b[0m \u001b[43mgame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m entity \u001b[38;5;129;01min\u001b[39;00m game\u001b[38;5;241m.\u001b[39mentities:\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m entity\u001b[38;5;241m.\u001b[39mchosen:\n",
      "File \u001b[1;32mc:\\projects\\predatorPrey\\game.py:173\u001b[0m, in \u001b[0;36mGame.play\u001b[1;34m(self, turns_max)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplay\u001b[39m(\u001b[38;5;28mself\u001b[39m, turns_max\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m):\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;66;03m# self.display.setup()\u001b[39;00m\n\u001b[0;32m    172\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m turns_max \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mturnNo:\n\u001b[1;32m--> 173\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mturn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    174\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mturnNo \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    176\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplay \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\projects\\predatorPrey\\game.py:155\u001b[0m, in \u001b[0;36mGame.turn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    153\u001b[0m     \u001b[38;5;66;03m# self.global_outlook = self.get_global_outlook()\u001b[39;00m\n\u001b[0;32m    154\u001b[0m     outlook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_outlook(entity)\n\u001b[1;32m--> 155\u001b[0m     \u001b[43mentity\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutlook\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_entity_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    156\u001b[0m     timer_outlook\u001b[38;5;241m.\u001b[39mtoc()\n\u001b[0;32m    159\u001b[0m timer_collisions\u001b[38;5;241m.\u001b[39mtic()\n",
      "File \u001b[1;32mc:\\projects\\predatorPrey\\entity.py:98\u001b[0m, in \u001b[0;36mEntity.act\u001b[1;34m(self, outlook, entityDict)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearner:\n\u001b[0;32m     97\u001b[0m     timer_fit\u001b[38;5;241m.\u001b[39mtic()\n\u001b[1;32m---> 98\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutlook\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentityDict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mentityDict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m     timer_fit\u001b[38;5;241m.\u001b[39mtoc()\n",
      "File \u001b[1;32mc:\\projects\\predatorPrey\\entity.py:138\u001b[0m, in \u001b[0;36mEntity.fit\u001b[1;34m(self, outlook, done, entityDict)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward_high_hp()\n\u001b[0;32m    137\u001b[0m \u001b[38;5;66;03m# self.reward_close_proximity(entityDict)\u001b[39;00m\n\u001b[1;32m--> 138\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqlearn_cyclic\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreward\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprevious_estimates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprevious_estimates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprevious_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprevious_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcurrent_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutlook\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutlook\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdone\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\projects\\predatorPrey\\neuralNetwork.py:133\u001b[0m, in \u001b[0;36mNeuralNetwork.qlearn_cyclic\u001b[1;34m(self, reward, previous_estimates, previous_state, current_state, done)\u001b[0m\n\u001b[0;32m    128\u001b[0m     target[previous_pick] \u001b[38;5;241m=\u001b[39m reward \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mglobals\u001b[39m\u001b[38;5;241m.\u001b[39mmodelParams_gamma \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(\n\u001b[0;32m    129\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(current_state)\n\u001b[0;32m    130\u001b[0m     )\n\u001b[0;32m    131\u001b[0m \u001b[38;5;66;03m#epsilon greedy has been moved to entity.perform move for implementation reasons\u001b[39;00m\n\u001b[1;32m--> 133\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprevious_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodelParams_learning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmseLoss\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\projects\\predatorPrey\\neuralNetwork.py:117\u001b[0m, in \u001b[0;36mNeuralNetwork.train\u001b[1;34m(self, input, target, learning_rate, lossFunction)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloadInput(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward()\n\u001b[1;32m--> 117\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackpropagation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlossFunction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\projects\\predatorPrey\\neuralNetwork.py:93\u001b[0m, in \u001b[0;36mNeuralNetwork.backpropagation\u001b[1;34m(self, target, learning_rate, lossFunction)\u001b[0m\n\u001b[0;32m     87\u001b[0m derivative \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivationFunctions[i](\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m], derivative\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     89\u001b[0m )\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# Calculate the error of the current layer\u001b[39;00m\n\u001b[0;32m     92\u001b[0m layer_error \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m---> 93\u001b[0m     \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_error\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mderivative\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m learning_rate\n\u001b[0;32m     94\u001b[0m )\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# Calculate the adjustments for the weights and biases\u001b[39;00m\n\u001b[0;32m     97\u001b[0m weight_adjustments \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mouter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[i], output_error)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "game = Game(\n",
    "    display=Display(),\n",
    "    learning=False,\n",
    ")\n",
    "\n",
    "for i in range(100):\n",
    "    timer_total.tic()\n",
    "    print(f\"Game {i}\")\n",
    "    game.setup()\n",
    "    game.play()\n",
    "\n",
    "    for entity in game.entities:\n",
    "        if entity.chosen:\n",
    "            specimen = entity\n",
    "    animals = [entity for entity in game.entities if type(entity) is not Grass]\n",
    "\n",
    "    # mkdir_if_not_exist(f\"learned{i}\")\n",
    "    # Sheep.brain.save(f\"learned{i}/sheep.pkl\")\n",
    "    # Wolf.brain.save(f\"learned{i}/wolf.pkl\")\n",
    "\n",
    "\n",
    "    timer_total.toc()\n",
    "\n",
    "game.display.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 9\n",
      "0 0\n"
     ]
    }
   ],
   "source": [
    "from utils import generate_outputs\n",
    "import numpy\n",
    "import cv2\n",
    "\n",
    "map = np.array(generate_outputs(specimen.speed)) + specimen.speed\n",
    "\n",
    "reward_list = specimen.brain.readOutput()\n",
    "\n",
    "size = int(specimen.speed * 2 + 1)\n",
    "predicted_rewards = np.zeros((size, size))\n",
    "for i in range(len(map)):\n",
    "    predicted_rewards[map[i][0], map[i][1]] = reward_list[i].astype(np.float32)\n",
    "# to rgb for display\n",
    "predicted_rewards = predicted_rewards + np.abs(np.min(predicted_rewards))\n",
    "predicted_rewards = (predicted_rewards / np.max(predicted_rewards))*255\n",
    "\n",
    "predicted_rewards = np.dstack(\n",
    "    (\n",
    "        predicted_rewards,\n",
    "        np.zeros(predicted_rewards.shape),\n",
    "        np.zeros(predicted_rewards.shape),\n",
    "    )\n",
    ")\n",
    "\n",
    "best_x, best_y, best_z = np.unravel_index(\n",
    "    np.argmax(predicted_rewards, axis=None), predicted_rewards.shape\n",
    ")\n",
    "\n",
    "a,b = specimen.last_move\n",
    "a += specimen.speed\n",
    "b += specimen.speed\n",
    "\n",
    "print(best_x, best_y)\n",
    "print(a,b)\n",
    "\n",
    "predicted_rewards[a, b] = [0, 255, 0]\n",
    "predicted_rewards[best_x, best_y] = [0, 0, 255]\n",
    "predicted_rewards = cv2.resize(predicted_rewards, predicted_rewards.shape[0:2]).astype(np.uint8)\n",
    "\n",
    "actions = generate_outputs(specimen.speed)\n",
    "np.array(actions[np.argmax(reward_list)])\n",
    "\n",
    "import cv2\n",
    "\n",
    "# predicted_rewards2 = np.flip(predicted_rewards)\n",
    "# predicted_rewards2 = np.transpose(predicted_rewards2, (1, 0, 2))\n",
    "cv2.namedWindow(\"rewards\", cv2.WINDOW_NORMAL)\n",
    "cv2.imshow('rewards', predicted_rewards)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "# len(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-6, -6)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "specimen.last_move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9995308 -0.99964255\n",
      "0.73498607 -0.82637453\n",
      "0.95414966 -0.9860512\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "215"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in Sheep.brain.layers:\n",
    "    print(i.max(), i.min())\n",
    "game.turnNo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir_if_not_exist(f\"mk{globals.version_no}\")\n",
    "\n",
    "Sheep.brain.save(f\"mk{globals.version_no}/sheepResult{1}.keras\")\n",
    "Wolf.brain.save(f\"mk{globals.version_no}/wolfResult{1}.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict:\n",
      "Total time: 2.748 s\n",
      "Total calls: 15784\n",
      "Average time: 0.00017410035478966044 s\n",
      "\n",
      "Total:\n",
      "Total time: 182.484 s\n",
      "Total calls: 7\n",
      "Average time: 26.069142857142857 s\n",
      "\n",
      "Fit:\n",
      "Total time: 19.74 s\n",
      "Total calls: 24444\n",
      "Average time: 0.0008075601374570446 s\n",
      "\n",
      "Outlook:\n",
      "Total time: 0.566 s\n",
      "Total calls: 24363\n",
      "Average time: 2.3231950088248572e-05 s\n",
      "\n",
      "Outlook global:\n",
      "Total time: 152.225 s\n",
      "Total calls: 1367\n",
      "Average time: 0.11135698610095099 s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Predict:\")\n",
    "timer_predict.stats()\n",
    "\n",
    "print(\"Total:\")\n",
    "timer_total.stats()\n",
    "\n",
    "print(\"Fit:\")\n",
    "timer_fit.stats()\n",
    "\n",
    "print(\"Outlook:\")\n",
    "timer_outlook.stats()\n",
    "\n",
    "print(\"Outlook global:\")\n",
    "timer_outlook_global.stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-4, -4),\n",
       " (-4, -3),\n",
       " (-4, -2),\n",
       " (-4, -1),\n",
       " (-4, 0),\n",
       " (-4, 1),\n",
       " (-4, 2),\n",
       " (-4, 3),\n",
       " (-4, 4),\n",
       " (-3, -4),\n",
       " (-3, -3),\n",
       " (-3, -2),\n",
       " (-3, -1),\n",
       " (-3, 0),\n",
       " (-3, 1),\n",
       " (-3, 2),\n",
       " (-3, 3),\n",
       " (-3, 4),\n",
       " (-2, -4),\n",
       " (-2, -3),\n",
       " (-2, -2),\n",
       " (-2, -1),\n",
       " (-2, 0),\n",
       " (-2, 1),\n",
       " (-2, 2),\n",
       " (-2, 3),\n",
       " (-2, 4),\n",
       " (-1, -4),\n",
       " (-1, -3),\n",
       " (-1, -2),\n",
       " (-1, -1),\n",
       " (-1, 0),\n",
       " (-1, 1),\n",
       " (-1, 2),\n",
       " (-1, 3),\n",
       " (-1, 4),\n",
       " (0, -4),\n",
       " (0, -3),\n",
       " (0, -2),\n",
       " (0, -1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 2),\n",
       " (0, 3),\n",
       " (0, 4),\n",
       " (1, -4),\n",
       " (1, -3),\n",
       " (1, -2),\n",
       " (1, -1),\n",
       " (1, 0),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 3),\n",
       " (1, 4),\n",
       " (2, -4),\n",
       " (2, -3),\n",
       " (2, -2),\n",
       " (2, -1),\n",
       " (2, 0),\n",
       " (2, 1),\n",
       " (2, 2),\n",
       " (2, 3),\n",
       " (2, 4),\n",
       " (3, -4),\n",
       " (3, -3),\n",
       " (3, -2),\n",
       " (3, -1),\n",
       " (3, 0),\n",
       " (3, 1),\n",
       " (3, 2),\n",
       " (3, 3),\n",
       " (3, 4),\n",
       " (4, -4),\n",
       " (4, -3),\n",
       " (4, -2),\n",
       " (4, -1),\n",
       " (4, 0),\n",
       " (4, 1),\n",
       " (4, 2),\n",
       " (4, 3),\n",
       " (4, 4)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
